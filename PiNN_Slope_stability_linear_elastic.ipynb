{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\apisi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\__init__.py:747: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ..\\torch\\csrc\\tensor\\python_tensor.cpp:433.)\n",
      "  _C._set_default_tensor_type(t)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/2000, Loss: 1.951164, LR: 0.001000\n",
      "Epoch 20/2000, Loss: 1.009543, LR: 0.001000\n",
      "Epoch 30/2000, Loss: 0.490592, LR: 0.000999\n",
      "Epoch 40/2000, Loss: 0.279909, LR: 0.000999\n",
      "Epoch 50/2000, Loss: 0.201430, LR: 0.000998\n",
      "Epoch 60/2000, Loss: 0.168948, LR: 0.000998\n",
      "Epoch 70/2000, Loss: 0.158938, LR: 0.000997\n",
      "Epoch 80/2000, Loss: 0.162598, LR: 0.000996\n",
      "Epoch 90/2000, Loss: 0.129876, LR: 0.000995\n",
      "Epoch 100/2000, Loss: 0.114328, LR: 0.000994\n",
      "Epoch 110/2000, Loss: 0.107493, LR: 0.000993\n",
      "Epoch 120/2000, Loss: 0.108331, LR: 0.000991\n",
      "Epoch 130/2000, Loss: 0.084107, LR: 0.000990\n",
      "Epoch 140/2000, Loss: 0.118304, LR: 0.000988\n",
      "Epoch 150/2000, Loss: 0.081557, LR: 0.000986\n",
      "Epoch 160/2000, Loss: 0.081014, LR: 0.000984\n",
      "Epoch 170/2000, Loss: 0.081164, LR: 0.000982\n",
      "Epoch 180/2000, Loss: 0.120704, LR: 0.000980\n",
      "Epoch 190/2000, Loss: 0.078863, LR: 0.000978\n",
      "Epoch 200/2000, Loss: 0.096029, LR: 0.000976\n",
      "Epoch 210/2000, Loss: 0.075433, LR: 0.000973\n",
      "Epoch 220/2000, Loss: 0.168653, LR: 0.000970\n",
      "Epoch 230/2000, Loss: 0.123425, LR: 0.000968\n",
      "Epoch 240/2000, Loss: 0.080530, LR: 0.000965\n",
      "Epoch 250/2000, Loss: 0.094512, LR: 0.000962\n",
      "Epoch 260/2000, Loss: 0.071146, LR: 0.000959\n",
      "Epoch 270/2000, Loss: 0.066546, LR: 0.000956\n",
      "Epoch 280/2000, Loss: 0.072245, LR: 0.000952\n",
      "Epoch 290/2000, Loss: 0.065351, LR: 0.000949\n",
      "Epoch 300/2000, Loss: 0.091011, LR: 0.000946\n",
      "Epoch 310/2000, Loss: 0.061663, LR: 0.000942\n",
      "Epoch 320/2000, Loss: 0.074169, LR: 0.000938\n",
      "Epoch 330/2000, Loss: 0.051554, LR: 0.000934\n",
      "Epoch 340/2000, Loss: 0.055590, LR: 0.000930\n",
      "Epoch 350/2000, Loss: 0.088607, LR: 0.000926\n",
      "Epoch 360/2000, Loss: 0.113226, LR: 0.000922\n",
      "Epoch 370/2000, Loss: 0.068394, LR: 0.000918\n",
      "Epoch 380/2000, Loss: 0.068546, LR: 0.000914\n",
      "Epoch 390/2000, Loss: 0.074079, LR: 0.000909\n",
      "Epoch 400/2000, Loss: 0.054091, LR: 0.000905\n",
      "Epoch 410/2000, Loss: 0.078687, LR: 0.000900\n",
      "Epoch 420/2000, Loss: 0.058626, LR: 0.000895\n",
      "Epoch 430/2000, Loss: 0.064811, LR: 0.000890\n",
      "Epoch 440/2000, Loss: 0.050349, LR: 0.000885\n",
      "Epoch 450/2000, Loss: 0.046699, LR: 0.000880\n",
      "Epoch 460/2000, Loss: 0.071852, LR: 0.000875\n",
      "Epoch 470/2000, Loss: 0.038925, LR: 0.000870\n",
      "Epoch 480/2000, Loss: 0.041358, LR: 0.000865\n",
      "Epoch 490/2000, Loss: 0.038131, LR: 0.000859\n",
      "Epoch 500/2000, Loss: 0.041330, LR: 0.000854\n",
      "Epoch 510/2000, Loss: 0.081673, LR: 0.000848\n",
      "Epoch 520/2000, Loss: 0.042140, LR: 0.000842\n",
      "Epoch 530/2000, Loss: 0.039182, LR: 0.000837\n",
      "Epoch 540/2000, Loss: 0.050527, LR: 0.000831\n",
      "Epoch 550/2000, Loss: 0.047553, LR: 0.000825\n",
      "Epoch 560/2000, Loss: 0.042593, LR: 0.000819\n",
      "Epoch 570/2000, Loss: 0.036121, LR: 0.000813\n",
      "Epoch 580/2000, Loss: 0.050837, LR: 0.000807\n",
      "Epoch 590/2000, Loss: 0.032716, LR: 0.000800\n",
      "Epoch 600/2000, Loss: 0.047375, LR: 0.000794\n",
      "Epoch 610/2000, Loss: 0.036390, LR: 0.000788\n",
      "Epoch 620/2000, Loss: 0.054960, LR: 0.000781\n",
      "Epoch 630/2000, Loss: 0.030559, LR: 0.000775\n",
      "Epoch 640/2000, Loss: 0.039892, LR: 0.000768\n",
      "Epoch 650/2000, Loss: 0.034234, LR: 0.000761\n",
      "Epoch 660/2000, Loss: 0.048252, LR: 0.000755\n",
      "Epoch 670/2000, Loss: 0.027647, LR: 0.000748\n",
      "Epoch 680/2000, Loss: 0.037792, LR: 0.000741\n",
      "Epoch 690/2000, Loss: 0.033008, LR: 0.000734\n",
      "Epoch 700/2000, Loss: 0.041555, LR: 0.000727\n",
      "Epoch 710/2000, Loss: 0.028680, LR: 0.000720\n",
      "Epoch 720/2000, Loss: 0.035154, LR: 0.000713\n",
      "Epoch 730/2000, Loss: 0.027479, LR: 0.000706\n",
      "Epoch 740/2000, Loss: 0.042918, LR: 0.000699\n",
      "Epoch 750/2000, Loss: 0.024307, LR: 0.000692\n",
      "Epoch 760/2000, Loss: 0.033497, LR: 0.000684\n",
      "Epoch 770/2000, Loss: 0.026089, LR: 0.000677\n",
      "Epoch 780/2000, Loss: 0.026322, LR: 0.000670\n",
      "Epoch 790/2000, Loss: 0.029122, LR: 0.000662\n",
      "Epoch 800/2000, Loss: 0.030642, LR: 0.000655\n",
      "Epoch 810/2000, Loss: 0.023805, LR: 0.000647\n",
      "Epoch 820/2000, Loss: 0.033271, LR: 0.000640\n",
      "Epoch 830/2000, Loss: 0.028989, LR: 0.000632\n",
      "Epoch 840/2000, Loss: 0.024133, LR: 0.000625\n",
      "Epoch 850/2000, Loss: 0.024794, LR: 0.000617\n",
      "Epoch 860/2000, Loss: 0.029303, LR: 0.000609\n",
      "Epoch 870/2000, Loss: 0.021393, LR: 0.000602\n",
      "Epoch 880/2000, Loss: 0.023647, LR: 0.000594\n",
      "Epoch 890/2000, Loss: 0.024149, LR: 0.000586\n",
      "Epoch 900/2000, Loss: 0.025260, LR: 0.000579\n",
      "Epoch 910/2000, Loss: 0.024520, LR: 0.000571\n",
      "Epoch 920/2000, Loss: 0.026172, LR: 0.000563\n",
      "Epoch 930/2000, Loss: 0.020054, LR: 0.000555\n",
      "Epoch 940/2000, Loss: 0.034561, LR: 0.000548\n",
      "Epoch 950/2000, Loss: 0.026794, LR: 0.000540\n",
      "Epoch 960/2000, Loss: 0.029354, LR: 0.000532\n",
      "Epoch 970/2000, Loss: 0.022397, LR: 0.000524\n",
      "Epoch 980/2000, Loss: 0.026033, LR: 0.000516\n",
      "Epoch 990/2000, Loss: 0.025570, LR: 0.000508\n",
      "Epoch 1000/2000, Loss: 0.025359, LR: 0.000500\n",
      "Epoch 1010/2000, Loss: 0.021570, LR: 0.000493\n",
      "Epoch 1020/2000, Loss: 0.024459, LR: 0.000485\n",
      "Epoch 1030/2000, Loss: 0.021687, LR: 0.000477\n",
      "Epoch 1040/2000, Loss: 0.020337, LR: 0.000469\n",
      "Epoch 1050/2000, Loss: 0.019872, LR: 0.000461\n",
      "Epoch 1060/2000, Loss: 0.019044, LR: 0.000453\n",
      "Epoch 1070/2000, Loss: 0.021425, LR: 0.000446\n",
      "Epoch 1080/2000, Loss: 0.022616, LR: 0.000438\n",
      "Epoch 1090/2000, Loss: 0.017099, LR: 0.000430\n",
      "Epoch 1100/2000, Loss: 0.018394, LR: 0.000422\n",
      "Epoch 1110/2000, Loss: 0.022398, LR: 0.000415\n",
      "Epoch 1120/2000, Loss: 0.016816, LR: 0.000407\n",
      "Epoch 1130/2000, Loss: 0.016099, LR: 0.000399\n",
      "Epoch 1140/2000, Loss: 0.023164, LR: 0.000392\n",
      "Epoch 1150/2000, Loss: 0.018856, LR: 0.000384\n",
      "Epoch 1160/2000, Loss: 0.019197, LR: 0.000376\n",
      "Epoch 1170/2000, Loss: 0.017731, LR: 0.000369\n",
      "Epoch 1180/2000, Loss: 0.018115, LR: 0.000361\n",
      "Epoch 1190/2000, Loss: 0.017071, LR: 0.000354\n",
      "Epoch 1200/2000, Loss: 0.016787, LR: 0.000346\n",
      "Epoch 1210/2000, Loss: 0.020491, LR: 0.000339\n",
      "Epoch 1220/2000, Loss: 0.018217, LR: 0.000331\n",
      "Epoch 1230/2000, Loss: 0.017384, LR: 0.000324\n",
      "Epoch 1240/2000, Loss: 0.017778, LR: 0.000317\n",
      "Epoch 1250/2000, Loss: 0.016264, LR: 0.000309\n",
      "Epoch 1260/2000, Loss: 0.017127, LR: 0.000302\n",
      "Epoch 1270/2000, Loss: 0.016126, LR: 0.000295\n",
      "Epoch 1280/2000, Loss: 0.017035, LR: 0.000288\n",
      "Epoch 1290/2000, Loss: 0.015533, LR: 0.000281\n",
      "Epoch 1300/2000, Loss: 0.017721, LR: 0.000274\n",
      "Epoch 1310/2000, Loss: 0.015135, LR: 0.000267\n",
      "Epoch 1320/2000, Loss: 0.018033, LR: 0.000260\n",
      "Epoch 1330/2000, Loss: 0.015353, LR: 0.000253\n",
      "Epoch 1340/2000, Loss: 0.015054, LR: 0.000246\n",
      "Epoch 1350/2000, Loss: 0.015269, LR: 0.000240\n",
      "Epoch 1360/2000, Loss: 0.015420, LR: 0.000233\n",
      "Epoch 1370/2000, Loss: 0.015787, LR: 0.000226\n",
      "Epoch 1380/2000, Loss: 0.015453, LR: 0.000220\n",
      "Epoch 1390/2000, Loss: 0.015341, LR: 0.000213\n",
      "Epoch 1400/2000, Loss: 0.015588, LR: 0.000207\n",
      "Epoch 1410/2000, Loss: 0.014971, LR: 0.000201\n",
      "Epoch 1420/2000, Loss: 0.015758, LR: 0.000194\n",
      "Epoch 1430/2000, Loss: 0.014702, LR: 0.000188\n",
      "Epoch 1440/2000, Loss: 0.015885, LR: 0.000182\n",
      "Epoch 1450/2000, Loss: 0.014560, LR: 0.000176\n",
      "Epoch 1460/2000, Loss: 0.014753, LR: 0.000170\n",
      "Epoch 1470/2000, Loss: 0.015260, LR: 0.000164\n",
      "Epoch 1480/2000, Loss: 0.015345, LR: 0.000159\n",
      "Epoch 1490/2000, Loss: 0.015186, LR: 0.000153\n",
      "Epoch 1500/2000, Loss: 0.014586, LR: 0.000147\n",
      "Epoch 1510/2000, Loss: 0.014574, LR: 0.000142\n",
      "Epoch 1520/2000, Loss: 0.014876, LR: 0.000136\n",
      "Epoch 1530/2000, Loss: 0.014390, LR: 0.000131\n",
      "Epoch 1540/2000, Loss: 0.014821, LR: 0.000126\n",
      "Epoch 1550/2000, Loss: 0.014301, LR: 0.000121\n",
      "Epoch 1560/2000, Loss: 0.014722, LR: 0.000116\n",
      "Epoch 1570/2000, Loss: 0.014293, LR: 0.000111\n",
      "Epoch 1580/2000, Loss: 0.014457, LR: 0.000106\n",
      "Epoch 1590/2000, Loss: 0.014289, LR: 0.000101\n",
      "Epoch 1600/2000, Loss: 0.014331, LR: 0.000096\n",
      "Epoch 1610/2000, Loss: 0.014223, LR: 0.000092\n",
      "Epoch 1620/2000, Loss: 0.014308, LR: 0.000087\n",
      "Epoch 1630/2000, Loss: 0.014135, LR: 0.000083\n",
      "Epoch 1640/2000, Loss: 0.014195, LR: 0.000079\n",
      "Epoch 1650/2000, Loss: 0.014108, LR: 0.000075\n",
      "Epoch 1660/2000, Loss: 0.014169, LR: 0.000071\n",
      "Epoch 1670/2000, Loss: 0.014066, LR: 0.000067\n",
      "Epoch 1680/2000, Loss: 0.013984, LR: 0.000063\n",
      "Epoch 1690/2000, Loss: 0.013966, LR: 0.000059\n",
      "Epoch 1700/2000, Loss: 0.013944, LR: 0.000055\n",
      "Epoch 1710/2000, Loss: 0.013928, LR: 0.000052\n",
      "Epoch 1720/2000, Loss: 0.013914, LR: 0.000049\n",
      "Epoch 1730/2000, Loss: 0.013901, LR: 0.000045\n",
      "Epoch 1740/2000, Loss: 0.013889, LR: 0.000042\n",
      "Epoch 1750/2000, Loss: 0.013877, LR: 0.000039\n",
      "Epoch 1760/2000, Loss: 0.013867, LR: 0.000036\n",
      "Epoch 1770/2000, Loss: 0.013857, LR: 0.000033\n",
      "Epoch 1780/2000, Loss: 0.013849, LR: 0.000031\n",
      "Epoch 1790/2000, Loss: 0.013841, LR: 0.000028\n",
      "Epoch 1800/2000, Loss: 0.013833, LR: 0.000025\n",
      "Epoch 1810/2000, Loss: 0.013827, LR: 0.000023\n",
      "Epoch 1820/2000, Loss: 0.013821, LR: 0.000021\n",
      "Epoch 1830/2000, Loss: 0.013816, LR: 0.000019\n",
      "Epoch 1840/2000, Loss: 0.013811, LR: 0.000017\n",
      "Epoch 1850/2000, Loss: 0.013806, LR: 0.000015\n",
      "Epoch 1860/2000, Loss: 0.013803, LR: 0.000013\n",
      "Epoch 1870/2000, Loss: 0.013799, LR: 0.000011\n",
      "Epoch 1880/2000, Loss: 0.013796, LR: 0.000010\n",
      "Epoch 1890/2000, Loss: 0.013794, LR: 0.000008\n",
      "Epoch 1900/2000, Loss: 0.013792, LR: 0.000007\n",
      "Epoch 1910/2000, Loss: 0.013790, LR: 0.000006\n",
      "Epoch 1920/2000, Loss: 0.013788, LR: 0.000005\n",
      "Epoch 1930/2000, Loss: 0.013787, LR: 0.000004\n",
      "Epoch 1940/2000, Loss: 0.013786, LR: 0.000003\n",
      "Epoch 1950/2000, Loss: 0.013785, LR: 0.000003\n",
      "Epoch 1960/2000, Loss: 0.013785, LR: 0.000002\n",
      "Epoch 1970/2000, Loss: 0.013784, LR: 0.000002\n",
      "Epoch 1980/2000, Loss: 0.013784, LR: 0.000001\n",
      "Epoch 1990/2000, Loss: 0.013783, LR: 0.000001\n",
      "Epoch 2000/2000, Loss: 0.013783, LR: 0.000001\n",
      "Results saved to PiNN2_data.csv\n",
      "Error in plot_results: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.path import Path\n",
    "import pandas as pd\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingLR\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set default tensor type to float32\n",
    "torch.set_default_tensor_type(torch.FloatTensor)\n",
    "\n",
    "# Define the neural network architecture\n",
    "class Net(nn.Module):\n",
    "       def __init__(self):\n",
    "           super(Net, self).__init__()\n",
    "           self.hidden1 = nn.Linear(2, 64)\n",
    "           self.hidden2 = nn.Linear(64, 128)\n",
    "           self.hidden3 = nn.Linear(128, 256)\n",
    "           self.hidden4 = nn.Linear(256, 128)\n",
    "           self.hidden5 = nn.Linear(128, 64)\n",
    "           self.output = nn.Linear(64, 1)\n",
    "\n",
    "       def forward(self, x):\n",
    "           x = torch.tanh(self.hidden1(x))\n",
    "           x = torch.tanh(self.hidden2(x))\n",
    "           x = torch.tanh(self.hidden3(x))\n",
    "           x = torch.tanh(self.hidden4(x))\n",
    "           x = torch.tanh(self.hidden5(x))\n",
    "           return self.output(x)\n",
    "    # Define the problem domain using the given vertices\n",
    "vertices = np.array([\n",
    "    [0, 0],\n",
    "    [0, 3],\n",
    "    [3, 3],\n",
    "    [5, 2],\n",
    "    [7, 2],\n",
    "    [7,0],\n",
    "    [0,0]  # Closing the polygon\n",
    "],dtype=np.float32)\n",
    "path = Path(vertices)\n",
    "def in_domain(x, y):\n",
    "    points = np.column_stack((x.cpu().numpy(), y.cpu().numpy()))\n",
    "    return torch.tensor(path.contains_points(points), dtype=torch.bool, device=device)\n",
    "\n",
    "# Define boundary conditions\n",
    "def BC_bottom(x, y):\n",
    "    return ((y == 0) & (x >= 0) & (x <= 7)).squeeze()\n",
    "\n",
    "def BC_left(x, y):\n",
    "    return ((x == 0) & (y >= 0) & (y <= 3)).squeeze()\n",
    "\n",
    "def BC_top(x, y):\n",
    "    return (((y == 3) & (x >= 0) & (x <= 3)) | \n",
    "            ((y - 4.5) * -2 == (x - 0)) & (x > 3) & (x < 5) |\n",
    "            (y == 2) & (x >= 5) & (x <= 7)).squeeze()\n",
    "\n",
    "def BC_right(x, y):\n",
    "    return ((x == 7) & (y >= 0) & (y <= 2)).squeeze()\n",
    "\n",
    "def BC_Slope(x, y):\n",
    "    return (((y - 4.5) * -2 == (x - 0)) & (x > 3) & (x < 5)).squeeze()\n",
    "\n",
    "def BC(xy, net_u, net_v):\n",
    "    x, y = xy[:, 0].unsqueeze(1), xy[:, 1].unsqueeze(1)\n",
    "    \n",
    "    u = net_u(xy)\n",
    "    v = net_v(xy)\n",
    "    \n",
    "    bc_b = BC_bottom(x, y)\n",
    "    bc_l = BC_left(x, y)\n",
    "    bc_t = BC_top(x, y)\n",
    "    bc_r = BC_right(x, y)\n",
    "    bc_slope = BC_Slope(x, y)\n",
    "    \n",
    "    loss = torch.mean(u[bc_b]**2 + v[bc_b]**2)  # ux = uy = 0 on bottom\n",
    "    loss += torch.mean(u[bc_l]**2)  # ux = 0 on left side\n",
    "    loss += torch.mean(u[bc_r]**2)  # uy = 0 on right side\n",
    "    \n",
    "    # Stress-free condition at the top (σyy = 0, σxy = 0, σxx = 0)\n",
    "    xy_top = xy[bc_t].requires_grad_(True)\n",
    "    u_top = net_u(xy_top)\n",
    "    v_top = net_v(xy_top)\n",
    "    \n",
    "    # Gradients for calculating stresses\n",
    "    u_x_top = torch.autograd.grad(u_top.sum(), xy_top, create_graph=True)[0][:, 0]\n",
    "    u_y_top = torch.autograd.grad(u_top.sum(), xy_top, create_graph=True)[0][:, 1]\n",
    "    v_x_top = torch.autograd.grad(v_top.sum(), xy_top, create_graph=True)[0][:, 0]\n",
    "    v_y_top = torch.autograd.grad(v_top.sum(), xy_top, create_graph=True)[0][:, 1]\n",
    "    \n",
    "    E = 5.0  # Young's modulus\n",
    "    nu = 0.3  # Poisson's ratio\n",
    "    \n",
    "    # Calculate stresses at the top\n",
    "    sigma_xx_top = (E / ((1 + nu) * (1 - 2 * nu))) * (u_x_top + nu * v_y_top)\n",
    "    sigma_yy_top = (E / ((1 + nu) * (1 - 2 * nu))) * (v_y_top + nu * u_x_top)\n",
    "    sigma_xy_top = E / (2 * (1 + nu)) * (u_y_top + v_x_top)\n",
    "    \n",
    "    # Add stress conditions to loss\n",
    "    loss += torch.mean(sigma_xy_top**2)  # σxy = 0 at the top\n",
    "    loss += torch.mean(sigma_xx_top**2)  # σxx = 0 at the top\n",
    "    loss += torch.mean(sigma_yy_top**2)  # σyy = 0 at the top\n",
    "    \n",
    "    # Right boundary conditions\n",
    "    xy_right = xy[bc_r].requires_grad_(True)\n",
    "    u_right = net_u(xy_right)\n",
    "    v_right = net_v(xy_right)\n",
    "    \n",
    "    u_y_right = torch.autograd.grad(u_right.sum(), xy_right, create_graph=True)[0][:, 1]\n",
    "    v_x_right = torch.autograd.grad(v_right.sum(), xy_right, create_graph=True)[0][:, 0]\n",
    "    \n",
    "    sigma_xy_right = E / (2 * (1 + nu)) * (u_y_right + v_x_right)\n",
    "    loss += torch.mean(sigma_xy_right**2)\n",
    "\n",
    "    # Left boundary conditions\n",
    "    xy_left = xy[bc_l].requires_grad_(True)\n",
    "    u_left = net_u(xy_left)\n",
    "    v_left = net_v(xy_left)\n",
    "    \n",
    "    u_y_left = torch.autograd.grad(u_left.sum(), xy_left, create_graph=True)[0][:, 1]\n",
    "    v_x_left = torch.autograd.grad(v_left.sum(), xy_left, create_graph=True)[0][:, 0]\n",
    "    \n",
    "    sigma_xy_left = E / (2 * (1 + nu)) * (u_y_left + v_x_left)\n",
    "    loss += torch.mean(sigma_xy_left**2)\n",
    "\n",
    "    return loss\n",
    "\n",
    "def load_fem_data(filename='FEM2_data.csv'):\n",
    "    df = pd.read_csv(filename)\n",
    "    x = torch.tensor(df['X'].values, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "    y = torch.tensor(df['Y'].values, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def generate_training_data(fem_data_file, n_boundary=2000):\n",
    "    x, y = load_fem_data(fem_data_file)\n",
    "    \n",
    "    # Keep only points inside the domain\n",
    "    mask = in_domain(x, y)\n",
    "    x, y = x[mask], y[mask]\n",
    "    \n",
    "    # Generate boundary points\n",
    "    t = torch.linspace(0, 1, n_boundary, device=device).unsqueeze(1)\n",
    "    \n",
    "    # Define the boundary segments\n",
    "    segments = [\n",
    "        ([0, 0, 0], [0, 3, 3]),  # Left boundary\n",
    "        ([0, 3, 5, 7, 7], [3, 3, 2, 2, 0]),  # Top and right boundary\n",
    "        ([7, 0], [0, 0])  # Bottom boundary\n",
    "    ]\n",
    "    \n",
    "    x_b = []\n",
    "    y_b = []\n",
    "    \n",
    "    for segment in segments:\n",
    "        x_seg = torch.tensor(np.interp(t.cpu().numpy(), np.linspace(0, 1, len(segment[0])), segment[0]), dtype=torch.float32, device=device)\n",
    "        y_seg = torch.tensor(np.interp(t.cpu().numpy(), np.linspace(0, 1, len(segment[1])), segment[1]), dtype=torch.float32, device=device)\n",
    "        x_b.append(x_seg)\n",
    "        y_b.append(y_seg)\n",
    "    \n",
    "    x_b = torch.cat(x_b)\n",
    "    y_b = torch.cat(y_b)\n",
    "    \n",
    "    return x, y, x_b, y_b\n",
    "\n",
    "\n",
    "def PDE(x, y, net_u, net_v):\n",
    "    xy = torch.cat([x, y], dim=1)\n",
    "    xy.requires_grad = True\n",
    "    \n",
    "    u = net_u(xy)\n",
    "    v = net_v(xy)\n",
    "    \n",
    "    u_x = torch.autograd.grad(u.sum(), xy, create_graph=True)[0][:, 0].unsqueeze(1)\n",
    "    u_y = torch.autograd.grad(u.sum(), xy, create_graph=True)[0][:, 1].unsqueeze(1)\n",
    "    v_x = torch.autograd.grad(v.sum(), xy, create_graph=True)[0][:, 0].unsqueeze(1)\n",
    "    v_y = torch.autograd.grad(v.sum(), xy, create_graph=True)[0][:, 1].unsqueeze(1)\n",
    "    \n",
    "    E = 5  # Young's modulus\n",
    "    nu = 0.3  # Poisson's ratio\n",
    "    gamma = 1.8\n",
    "    sigma_xx = E / (1 - nu**2) * (u_x + nu * v_y)\n",
    "    sigma_yy = E / (1 - nu**2) * (v_y + nu * u_x)\n",
    "    sigma_xy = E / (2 * (1 + nu)) * (u_y + v_x)\n",
    "    \n",
    "    f_x = torch.zeros_like(x)\n",
    "    f_y = -gamma * torch.ones_like(y)  # Body force\n",
    "    \n",
    "    R_x = torch.autograd.grad(sigma_xx.sum(), xy, create_graph=True)[0][:, 0].unsqueeze(1) + \\\n",
    "          torch.autograd.grad(sigma_xy.sum(), xy, create_graph=True)[0][:, 1].unsqueeze(1) + f_x\n",
    "    R_y = torch.autograd.grad(sigma_xy.sum(), xy, create_graph=True)[0][:, 0].unsqueeze(1) + \\\n",
    "          torch.autograd.grad(sigma_yy.sum(), xy, create_graph=True)[0][:, 1].unsqueeze(1) + f_y\n",
    "    \n",
    "    loss_x = torch.mean(R_x**2)\n",
    "    loss_y = torch.mean(R_y**2)\n",
    "    \n",
    "    return loss_x, loss_y\n",
    "\n",
    "\n",
    "def train(net_u, net_v, optimizer, n_epochs, fem_data_file):\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=n_epochs, eta_min=1e-6)\n",
    "    \n",
    "    best_loss = float('inf')\n",
    "    patience = 0\n",
    "    max_patience =3000\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        x, y, x_b, y_b = generate_training_data(fem_data_file, 30000)\n",
    "        xy = torch.cat([x, y], dim=1)\n",
    "        xy_b = torch.cat([x_b, y_b], dim=1)\n",
    "        \n",
    "        loss_pde_x, loss_pde_y = PDE(x, y, net_u, net_v)\n",
    "        loss_bc = BC(xy_b, net_u, net_v)\n",
    "        \n",
    "        loss = loss_pde_x + 2 * loss_pde_y + loss_bc\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(list(net_u.parameters()) + list(net_v.parameters()), max_norm=0.5)\n",
    "        optimizer.step()\n",
    "        scheduler.step()  # ไม่ต้องส่งค่า epoch เข้าไป\n",
    "        \n",
    "        if loss < best_loss:\n",
    "            best_loss = loss\n",
    "            patience = 0\n",
    "            torch.save({\n",
    "                'net_u_state_dict': net_u.state_dict(),\n",
    "                'net_v_state_dict': net_v.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),  # บันทึกสถานะของ scheduler ด้วย\n",
    "                'loss': loss,\n",
    "                'epoch': epoch,\n",
    "            }, 'best_model.pth')\n",
    "        else:\n",
    "            patience += 1\n",
    "            if patience > max_patience:\n",
    "                print(f\"Early stopping at epoch {epoch}\")\n",
    "                break\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch {epoch+1}/{n_epochs}, Loss: {loss.item():.6f}, LR: {scheduler.get_last_lr()[0]:.6f}')\n",
    "\n",
    "def plot_results(net_u, net_v, fem_data_file='FEM2_data.csv', output_filename='PiNN2_data.csv'):\n",
    "    # Load FEM data\n",
    "    df_fem = pd.read_csv(fem_data_file)\n",
    "    X = df_fem['X'].values\n",
    "    Y = df_fem['Y'].values\n",
    "    \n",
    "    # Create tensor from FEM data\n",
    "    XY = torch.tensor(np.column_stack([X, Y]), dtype=torch.float32).to(device)\n",
    "    \n",
    "    # Compute displacements and stresses for all points\n",
    "    XY.requires_grad_(True)\n",
    "    with torch.enable_grad():\n",
    "        U = net_u(XY)\n",
    "        V = net_v(XY)\n",
    "        \n",
    "        U_x = torch.autograd.grad(U.sum(), XY, create_graph=True)[0][:, 0]\n",
    "        U_y = torch.autograd.grad(U.sum(), XY, create_graph=True)[0][:, 1]\n",
    "        V_x = torch.autograd.grad(V.sum(), XY, create_graph=True)[0][:, 0]\n",
    "        V_y = torch.autograd.grad(V.sum(), XY, create_graph=True)[0][:, 1]\n",
    "        \n",
    "        E = 5  # Young's modulus\n",
    "        nu = 0.3  # Poisson's ratio\n",
    "        \n",
    "        sigma_xx = E / (1 - nu**2) * (U_x + nu * V_y)\n",
    "        sigma_yy = E / (1 - nu**2) * (V_y + nu * U_x)\n",
    "        sigma_xy = E / (2 * (1 + nu)) * (U_y + V_x)\n",
    "    \n",
    "    # Move tensors to CPU and convert to numpy\n",
    "    U_full = U.detach().cpu().numpy().squeeze()/1000\n",
    "    V_full = V.detach().cpu().numpy().squeeze()/1000\n",
    "    sigma_xx_full = sigma_xx.detach().cpu().numpy().squeeze()*10\n",
    "    sigma_yy_full = sigma_yy.detach().cpu().numpy().squeeze()*10\n",
    "    sigma_xy_full = sigma_xy.detach().cpu().numpy().squeeze()*10\n",
    "    \n",
    "    # Calculate displacement magnitude\n",
    "    magnitude = np.sqrt(U_full**2 + V_full**2)\n",
    "    \n",
    "    # Create DataFrame for CSV export\n",
    "    df_out = pd.DataFrame({\n",
    "        'X': X,\n",
    "        'Y': Y,\n",
    "        'ux': U_full,\n",
    "        'uy': V_full,\n",
    "        'sigma_xx': sigma_xx_full,\n",
    "        'sigma_yy': sigma_yy_full,\n",
    "        'sigma_xy': sigma_xy_full,\n",
    "        'magnitude': magnitude\n",
    "    })\n",
    "\n",
    "    # Save the output to a CSV file\n",
    "    df_out.to_csv(output_filename, index=False)\n",
    "    print(f\"Results saved to {output_filename}\")\n",
    "    \n",
    "    # Apply domain mask for plotting\n",
    "    mask = in_domain(XY[:, 0], XY[:, 1])\n",
    "    mask_cpu = mask.cpu().numpy()\n",
    "    \n",
    "    # Create masked arrays for plotting\n",
    "    U_masked = np.ma.masked_array(U_full, mask=~mask_cpu)\n",
    "    V_masked = np.ma.masked_array(V_full, mask=~mask_cpu)\n",
    "    sigma_xx_masked = np.ma.masked_array(sigma_xx_full, mask=~mask_cpu)\n",
    "    sigma_yy_masked = np.ma.masked_array(sigma_yy_full, mask=~mask_cpu)\n",
    "    sigma_xy_masked = np.ma.masked_array(sigma_xy_full, mask=~mask_cpu)\n",
    "    magnitude_masked = np.ma.masked_array(magnitude, mask=~mask_cpu)\n",
    "    \n",
    "    # Plot the results\n",
    "    plt.figure(figsize=(20, 15))\n",
    "    \n",
    "    # Plot u displacement (ux)\n",
    "    plt.subplot(231)\n",
    "    sc = plt.scatter(X, Y, c=U_masked, cmap='jet')\n",
    "    plt.colorbar(sc, label='ux displacement')\n",
    "    plt.title('ux displacement')\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Y')\n",
    "    \n",
    "    # Plot v displacement (uy)\n",
    "    plt.subplot(232)\n",
    "    sc = plt.scatter(X, Y, c=V_masked, cmap='jet')\n",
    "    plt.colorbar(sc, label='uy displacement')\n",
    "    plt.title('uy displacement')\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Y')\n",
    "    \n",
    "    # Plot sigma_xx\n",
    "    plt.subplot(233)\n",
    "    sc = plt.scatter(X, Y, c=sigma_xx_masked, cmap='jet')\n",
    "    plt.colorbar(sc, label='sigma_xx')\n",
    "    plt.title('sigma_xx')\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Y')\n",
    "    \n",
    "    # Plot sigma_yy\n",
    "    plt.subplot(234)\n",
    "    sc = plt.scatter(X, Y, c=sigma_yy_masked, cmap='jet')\n",
    "    plt.colorbar(sc, label='sigma_yy')\n",
    "    plt.title('sigma_yy')\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Y')\n",
    "    \n",
    "    # Plot sigma_xy\n",
    "    plt.subplot(235)\n",
    "    sc = plt.scatter(X, Y, c=sigma_xy_masked, cmap='jet')\n",
    "    plt.colorbar(sc, label='sigma_xy')\n",
    "    plt.title('sigma_xy')\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Y')\n",
    "    \n",
    "    # Plot displacement magnitude\n",
    "    plt.subplot(236)\n",
    "    sc = plt.scatter(X, Y, c=magnitude_masked, cmap='jet')\n",
    "    plt.colorbar(sc, label='Displacement magnitude')\n",
    "    plt.title('Displacement magnitude')\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Y')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('PiNN_results.png')\n",
    "    plt.show()\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    net_u = Net().to(device)\n",
    "    net_v = Net().to(device)\n",
    "    \n",
    "    optimizer = optim.Adam(list(net_u.parameters()) + list(net_v.parameters()), lr=0.001)\n",
    "    \n",
    "    fem_data_file = 'FEM2_data.csv'\n",
    "    train(net_u, net_v, optimizer, n_epochs=2000, fem_data_file=fem_data_file)\n",
    "    \n",
    "    # Load the best model\n",
    "    checkpoint = torch.load('best_model.pth')\n",
    "    net_u.load_state_dict(checkpoint['net_u_state_dict'])\n",
    "    net_v.load_state_dict(checkpoint['net_v_state_dict'])\n",
    "    \n",
    "    try:\n",
    "        plot_results(net_u, net_v, fem_data_file=fem_data_file)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in plot_results: {str(e)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
