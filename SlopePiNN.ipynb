{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\apisi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\__init__.py:747: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ..\\torch\\csrc\\tensor\\python_tensor.cpp:433.)\n",
      "  _C._set_default_tensor_type(t)\n",
      "c:\\Users\\apisi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/1000, Loss: 0.221831, LR: 0.001000\n",
      "Epoch 200/1000, Loss: 0.149411, LR: 0.001000\n",
      "Epoch 300/1000, Loss: 0.040693, LR: 0.001000\n",
      "Epoch 400/1000, Loss: 0.031763, LR: 0.001000\n",
      "Epoch 500/1000, Loss: 0.037954, LR: 0.001000\n",
      "Epoch 600/1000, Loss: 0.033611, LR: 0.001000\n",
      "Epoch 700/1000, Loss: 0.017168, LR: 0.001000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 357\u001b[0m\n\u001b[0;32m    354\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(\u001b[38;5;28mlist\u001b[39m(net_u\u001b[38;5;241m.\u001b[39mparameters()) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(net_v\u001b[38;5;241m.\u001b[39mparameters()), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n\u001b[0;32m    356\u001b[0m fem_data_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFEM2_data.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 357\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet_u\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnet_v\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfem_data_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfem_data_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    359\u001b[0m \u001b[38;5;66;03m# Load the best model\u001b[39;00m\n\u001b[0;32m    360\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_model.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[1], line 210\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(net_u, net_v, optimizer, n_epochs, fem_data_file)\u001b[0m\n\u001b[0;32m    206\u001b[0m loss_bc \u001b[38;5;241m=\u001b[39m BC(xy_b, net_u, net_v)\n\u001b[0;32m    208\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_pde_x \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m loss_pde_y \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m*\u001b[39m loss_bc\n\u001b[1;32m--> 210\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    211\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(\u001b[38;5;28mlist\u001b[39m(net_u\u001b[38;5;241m.\u001b[39mparameters()) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(net_v\u001b[38;5;241m.\u001b[39mparameters()), max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\n\u001b[0;32m    212\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\apisi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    524\u001b[0m     )\n\u001b[1;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\apisi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\apisi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.path import Path\n",
    "import pandas as pd\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set default tensor type to float32\n",
    "torch.set_default_tensor_type(torch.FloatTensor)\n",
    "\n",
    "# Define the neural network architecture\n",
    "class Net(nn.Module):\n",
    "       def __init__(self):\n",
    "           super(Net, self).__init__()\n",
    "           self.hidden1 = nn.Linear(2, 64)\n",
    "           self.hidden2 = nn.Linear(64, 128)\n",
    "           self.hidden3 = nn.Linear(128, 256)\n",
    "           self.hidden4 = nn.Linear(256, 128)\n",
    "           self.hidden5 = nn.Linear(128, 64)\n",
    "           self.output = nn.Linear(64, 1)\n",
    "\n",
    "       def forward(self, x):\n",
    "           x = torch.tanh(self.hidden1(x))\n",
    "           x = torch.tanh(self.hidden2(x))\n",
    "           x = torch.tanh(self.hidden3(x))\n",
    "           x = torch.tanh(self.hidden4(x))\n",
    "           x = torch.tanh(self.hidden5(x))\n",
    "           return self.output(x)\n",
    "    # Define the problem domain using the given vertices\n",
    "vertices = np.array([\n",
    "    [0, 0],\n",
    "    [0, 3],\n",
    "    [3, 3],\n",
    "    [5, 2],\n",
    "    [7, 2],\n",
    "    [7,0],\n",
    "    [0,0]  # Closing the polygon\n",
    "],dtype=np.float32)\n",
    "path = Path(vertices)\n",
    "def in_domain(x, y):\n",
    "    points = np.column_stack((x.cpu().numpy(), y.cpu().numpy()))\n",
    "    return torch.tensor(path.contains_points(points), dtype=torch.bool, device=device)\n",
    "\n",
    "# Define boundary conditions\n",
    "def BC_bottom(x, y):\n",
    "    return ((y == 0) & (x >= 0) & (x <= 7)).squeeze()\n",
    "\n",
    "def BC_left(x, y):\n",
    "    return ((x == 0) & (y >= 0) & (y <= 3)).squeeze()\n",
    "\n",
    "def BC_top(x, y):\n",
    "    return (((y == 3) & (x >= 0) & (x <= 3)) | \n",
    "            ((y - 2) / (-1) == (x - 3) / 2) & (x > 3) & (x < 5) |\n",
    "            (y == 2) & (x >= 5) & (x <= 7)).squeeze()\n",
    "\n",
    "def BC_right(x, y):\n",
    "    return ((x == 7) & (y >= 0) & (y <= 2)).squeeze()\n",
    "\n",
    "def BC(xy, net_u, net_v):\n",
    "    x, y = xy[:, 0].unsqueeze(1), xy[:, 1].unsqueeze(1)\n",
    "    \n",
    "    u = net_u(xy)\n",
    "    v = net_v(xy)\n",
    "    \n",
    "    bc_b = BC_bottom(x, y)\n",
    "    bc_l = BC_left(x, y)\n",
    "    bc_t = BC_top(x, y)\n",
    "    bc_r = BC_right(x, y)\n",
    "    \n",
    "    loss = torch.mean(u[bc_b]**2 + v[bc_b]**2)  # ux = uy = 0 on bottom\n",
    "    loss += torch.mean(u[bc_l]**2)  # ux = 0 on left side\n",
    "    loss += torch.mean(u[bc_r]**2)  # uy = 0 on right side\n",
    "    \n",
    "    # Stress-free condition at the top (σyy = 0, σxy = 0)\n",
    "    xy_top = xy[bc_t].requires_grad_(True)\n",
    "    u_top = net_u(xy_top)\n",
    "    v_top = net_v(xy_top)\n",
    "    \n",
    "    u_x_top = torch.autograd.grad(u_top.sum(), xy_top, create_graph=True)[0][:, 0]\n",
    "    u_y_top = torch.autograd.grad(u_top.sum(), xy_top, create_graph=True)[0][:, 1]\n",
    "    v_x_top = torch.autograd.grad(v_top.sum(), xy_top, create_graph=True)[0][:, 0]\n",
    "    v_y_top = torch.autograd.grad(v_top.sum(), xy_top, create_graph=True)[0][:, 1]\n",
    "    \n",
    "    E = 5.0  # Young's modulus\n",
    "    nu = 0.3  # Poisson's ratio\n",
    "    \n",
    "    sigma_yy_top = E / (1 - nu**2) * (v_y_top + nu * u_x_top)\n",
    "    sigma_xy_top = E / (2 * (1 + nu)) * (u_y_top + v_x_top)\n",
    "    \n",
    "    loss += torch.mean(sigma_yy_top**2 + sigma_xy_top**2)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def load_fem_data(filename='FEM2_data.csv'):\n",
    "    df = pd.read_csv(filename)\n",
    "    x = torch.tensor(df['X'].values, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "    y = torch.tensor(df['Y'].values, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "    return x, y\n",
    "\n",
    "# Generate training data\n",
    "def generate_training_data(fem_data_file, n_boundary):\n",
    "    x, y = load_fem_data(fem_data_file)\n",
    "    \n",
    "    # Keep only points inside the domain\n",
    "    mask = in_domain(x, y)\n",
    "    x, y = x[mask], y[mask]\n",
    "    \n",
    "    # Generate boundary points\n",
    "    t = torch.linspace(0, 1, n_boundary, device=device).unsqueeze(1)\n",
    "    \n",
    "    # Define the boundary segments\n",
    "    segments = [\n",
    "        ([0, 0, 0], [0, 3, 3]),  # Left boundary\n",
    "        ([0, 3, 5, 7, 7], [3, 3, 2, 2, 0]),  # Top and right boundary\n",
    "        ([7, 0], [0, 0])  # Bottom boundary\n",
    "    ]\n",
    "    \n",
    "    x_b = []\n",
    "    y_b = []\n",
    "    \n",
    "    for segment in segments:\n",
    "        x_seg = torch.tensor(np.interp(t.cpu().numpy(), np.linspace(0, 1, len(segment[0])), segment[0]), dtype=torch.float32, device=device)\n",
    "        y_seg = torch.tensor(np.interp(t.cpu().numpy(), np.linspace(0, 1, len(segment[1])), segment[1]), dtype=torch.float32, device=device)\n",
    "        x_b.append(x_seg)\n",
    "        y_b.append(y_seg)\n",
    "    \n",
    "    x_b = torch.cat(x_b)\n",
    "    y_b = torch.cat(y_b)\n",
    "    \n",
    "    return x, y, x_b, y_b\n",
    "\n",
    "def PDE(x, y, net_u, net_v):\n",
    "    xy = torch.cat([x, y], dim=1)\n",
    "    xy.requires_grad = True\n",
    "    \n",
    "    u = net_u(xy)\n",
    "    v = net_v(xy)\n",
    "    \n",
    "    u_x = torch.autograd.grad(u.sum(), xy, create_graph=True)[0][:, 0].unsqueeze(1)\n",
    "    u_y = torch.autograd.grad(u.sum(), xy, create_graph=True)[0][:, 1].unsqueeze(1)\n",
    "    v_x = torch.autograd.grad(v.sum(), xy, create_graph=True)[0][:, 0].unsqueeze(1)\n",
    "    v_y = torch.autograd.grad(v.sum(), xy, create_graph=True)[0][:, 1].unsqueeze(1)\n",
    "    \n",
    "    E = 5  # Young's modulus\n",
    "    nu = 0.3  # Poisson's ratio\n",
    "    gamma = 1\n",
    "    sigma_xx = E / (1 - nu**2) * (u_x + nu * v_y)\n",
    "    sigma_yy = E / (1 - nu**2) * (v_y + nu * u_x)\n",
    "    sigma_xy = E / (2 * (1 + nu)) * (u_y + v_x)\n",
    "    \n",
    "    f_x = torch.zeros_like(x)\n",
    "    f_y = -gamma * torch.ones_like(y)  # Body force\n",
    "    \n",
    "    R_x = torch.autograd.grad(sigma_xx.sum(), xy, create_graph=True)[0][:, 0].unsqueeze(1) + \\\n",
    "          torch.autograd.grad(sigma_xy.sum(), xy, create_graph=True)[0][:, 1].unsqueeze(1) + f_x\n",
    "    R_y = torch.autograd.grad(sigma_xy.sum(), xy, create_graph=True)[0][:, 0].unsqueeze(1) + \\\n",
    "          torch.autograd.grad(sigma_yy.sum(), xy, create_graph=True)[0][:, 1].unsqueeze(1) + f_y\n",
    "    \n",
    "    loss_x = torch.mean(R_x**2)\n",
    "    loss_y = torch.mean(R_y**2)\n",
    "    \n",
    "    return loss_x, loss_y\n",
    "\n",
    "\n",
    "def train(net_u, net_v, optimizer, n_epochs, fem_data_file):\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=500, verbose=True)\n",
    "    \n",
    "    best_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        x, y, x_b, y_b = generate_training_data(fem_data_file, 30000)\n",
    "        xy = torch.cat([x, y], dim=1)\n",
    "        xy_b = torch.cat([x_b, y_b], dim=1)\n",
    "        \n",
    "        loss_pde_x, loss_pde_y = PDE(x, y, net_u, net_v)\n",
    "        loss_bc = BC(xy_b, net_u, net_v)\n",
    "        \n",
    "        loss = loss_pde_x + 2 * loss_pde_y + 10 * loss_bc\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(list(net_u.parameters()) + list(net_v.parameters()), max_norm=0.5)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Step the scheduler\n",
    "        scheduler.step(loss)\n",
    "        \n",
    "        if loss < best_loss:\n",
    "            best_loss = loss\n",
    "            torch.save({\n",
    "                'net_u_state_dict': net_u.state_dict(),\n",
    "                'net_v_state_dict': net_v.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': loss,\n",
    "            }, 'best_model.pth')\n",
    "        \n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            print(f'Epoch {epoch+1}/{n_epochs}, Loss: {loss.item():.6f}, LR: {optimizer.param_groups[0][\"lr\"]:.6f}')\n",
    "\n",
    "def plot_results(net_u, net_v, fem_data_file='FEM2_data.csv', output_filename='PiNN2_data.csv'):\n",
    "    # Load FEM data\n",
    "    df_fem = pd.read_csv(fem_data_file)\n",
    "    X = df_fem['X'].values\n",
    "    Y = df_fem['Y'].values\n",
    "    \n",
    "    # Create tensor from FEM data\n",
    "    XY = torch.tensor(np.column_stack([X, Y]), dtype=torch.float32).to(device)\n",
    "    \n",
    "    # Compute displacements and stresses for all points\n",
    "    XY.requires_grad_(True)\n",
    "    with torch.enable_grad():\n",
    "        U = net_u(XY)\n",
    "        V = net_v(XY)\n",
    "        \n",
    "        U_x = torch.autograd.grad(U.sum(), XY, create_graph=True)[0][:, 0]\n",
    "        U_y = torch.autograd.grad(U.sum(), XY, create_graph=True)[0][:, 1]\n",
    "        V_x = torch.autograd.grad(V.sum(), XY, create_graph=True)[0][:, 0]\n",
    "        V_y = torch.autograd.grad(V.sum(), XY, create_graph=True)[0][:, 1]\n",
    "        \n",
    "        E = 5  # Young's modulus\n",
    "        nu = 0.3  # Poisson's ratio\n",
    "        \n",
    "        sigma_xx = E / (1 - nu**2) * (U_x + nu * V_y)\n",
    "        sigma_yy = E / (1 - nu**2) * (V_y + nu * U_x)\n",
    "        sigma_xy = E / (2 * (1 + nu)) * (U_y + V_x)\n",
    "    \n",
    "    # Move tensors to CPU and convert to numpy\n",
    "    U_full = U.detach().cpu().numpy().squeeze()\n",
    "    V_full = V.detach().cpu().numpy().squeeze()\n",
    "    sigma_xx_full = sigma_xx.detach().cpu().numpy().squeeze()\n",
    "    sigma_yy_full = sigma_yy.detach().cpu().numpy().squeeze()\n",
    "    sigma_xy_full = sigma_xy.detach().cpu().numpy().squeeze()\n",
    "    \n",
    "    # Calculate displacement magnitude\n",
    "    magnitude = np.sqrt(U_full**2 + V_full**2)\n",
    "    \n",
    "    # Create DataFrame for CSV export\n",
    "    df_out = pd.DataFrame({\n",
    "        'X': X,\n",
    "        'Y': Y,\n",
    "        'ux': U_full,\n",
    "        'uy': V_full,\n",
    "        'sigma_xx': sigma_xx_full,\n",
    "        'sigma_yy': sigma_yy_full,\n",
    "        'sigma_xy': sigma_xy_full,\n",
    "        'magnitude': magnitude\n",
    "    })\n",
    "\n",
    "    # Save the output to a CSV file\n",
    "    df_out.to_csv(output_filename, index=False)\n",
    "    print(f\"Results saved to {output_filename}\")\n",
    "    \n",
    "    # Apply domain mask for plotting\n",
    "    mask = in_domain(XY[:, 0], XY[:, 1])\n",
    "    mask_cpu = mask.cpu().numpy()\n",
    "    \n",
    "    # Create masked arrays for plotting\n",
    "    U_masked = np.ma.masked_array(U_full, mask=~mask_cpu)\n",
    "    V_masked = np.ma.masked_array(V_full, mask=~mask_cpu)\n",
    "    sigma_xx_masked = np.ma.masked_array(sigma_xx_full, mask=~mask_cpu)\n",
    "    sigma_yy_masked = np.ma.masked_array(sigma_yy_full, mask=~mask_cpu)\n",
    "    sigma_xy_masked = np.ma.masked_array(sigma_xy_full, mask=~mask_cpu)\n",
    "    magnitude_masked = np.ma.masked_array(magnitude, mask=~mask_cpu)\n",
    "    \n",
    "    # Plot the results\n",
    "    plt.figure(figsize=(20, 15))\n",
    "    \n",
    "    # Plot u displacement (ux)\n",
    "    plt.subplot(231)\n",
    "    sc = plt.scatter(X, Y, c=U_masked, cmap='jet')\n",
    "    plt.colorbar(sc, label='ux displacement')\n",
    "    plt.title('ux displacement')\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Y')\n",
    "    \n",
    "    # Plot v displacement (uy)\n",
    "    plt.subplot(232)\n",
    "    sc = plt.scatter(X, Y, c=V_masked, cmap='jet')\n",
    "    plt.colorbar(sc, label='uy displacement')\n",
    "    plt.title('uy displacement')\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Y')\n",
    "    \n",
    "    # Plot sigma_xx\n",
    "    plt.subplot(233)\n",
    "    sc = plt.scatter(X, Y, c=sigma_xx_masked, cmap='jet')\n",
    "    plt.colorbar(sc, label='sigma_xx')\n",
    "    plt.title('sigma_xx')\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Y')\n",
    "    \n",
    "    # Plot sigma_yy\n",
    "    plt.subplot(234)\n",
    "    sc = plt.scatter(X, Y, c=sigma_yy_masked, cmap='jet')\n",
    "    plt.colorbar(sc, label='sigma_yy')\n",
    "    plt.title('sigma_yy')\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Y')\n",
    "    \n",
    "    # Plot sigma_xy\n",
    "    plt.subplot(235)\n",
    "    sc = plt.scatter(X, Y, c=sigma_xy_masked, cmap='jet')\n",
    "    plt.colorbar(sc, label='sigma_xy')\n",
    "    plt.title('sigma_xy')\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Y')\n",
    "    \n",
    "    # Plot displacement magnitude\n",
    "    plt.subplot(236)\n",
    "    sc = plt.scatter(X, Y, c=magnitude_masked, cmap='jet')\n",
    "    plt.colorbar(sc, label='Displacement magnitude')\n",
    "    plt.title('Displacement magnitude')\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Y')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('PiNN_results.png')\n",
    "    plt.show()\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    net_u = Net().to(device)\n",
    "    net_v = Net().to(device)\n",
    "    \n",
    "    optimizer = optim.Adam(list(net_u.parameters()) + list(net_v.parameters()), lr=0.001)\n",
    "    \n",
    "    fem_data_file = 'FEM2_data.csv'\n",
    "    train(net_u, net_v, optimizer, n_epochs=1000, fem_data_file=fem_data_file)\n",
    "    \n",
    "    # Load the best model\n",
    "    checkpoint = torch.load('best_model.pth')\n",
    "    net_u.load_state_dict(checkpoint['net_u_state_dict'])\n",
    "    net_v.load_state_dict(checkpoint['net_v_state_dict'])\n",
    "    \n",
    "    try:\n",
    "        plot_results(net_u, net_v, fem_data_file=fem_data_file)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in plot_results: {str(e)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
